# -*- coding: utf-8 -*-
"""Data Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hwZrQEwwpYum3mhMbhIIpQJKWfQZq-tP

# Data Preprocessing for Psych8k Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# OpenAI API Key
openai.api_key = "your_openai_api_key_here"

# Step 1: Load the Psych8k dataset and inspect
import pandas as pd

# Define the dataset path
psych8k_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/Psych8k/Psych8k.csv"

# Load the dataset
psych8k_df = pd.read_csv(psych8k_path)

# Display the first few rows to understand the structure
print("Dataset Loaded Successfully!")
psych8k_df.head()

# Step 2: Validate and clean the Psych8k dataset

# Check for missing values
missing_values = psych8k_df.isnull().sum()
print("Missing Values in Each Column:\n", missing_values)

# Drop rows with missing values
psych8k_df = psych8k_df.dropna()

# Remove duplicates
psych8k_df = psych8k_df.drop_duplicates()

# Verify dataset shape after cleaning
print(f"Dataset shape after cleaning: {psych8k_df.shape}")

pip install openai==0.28

# STEP 1: Install required library
# !pip install openai --quiet

# STEP 2: Import libraries
import pandas as pd
import openai
import ast
import re
import time
from tqdm import tqdm

# STEP 3: Set your OpenAI API Key (replace with your own)
openai.api_key = "sk-BQeOeWHdHJvS4uJYMkf76e0oUYgrxeZM4WNk6BNZS5T3BlbkFJEt3ksOTNYzvY5i6UIH47ykF2rS1-7lzgL0zWhzh1IA"  # <- 🔐 Set securely via environment variable in production

# STEP 4: Load only the first 100 rows for testing
psych8k_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/Psych8k/Psych8k.csv"
df = pd.read_csv(psych8k_path)

# STEP 5: Function to get cognitive distortions from GPT-4o-mini
def get_distortions_batch(batch_inputs):
    prompt = (
        "You are a mental health assistant trained in Cognitive Behavioral Therapy (CBT). "
        "For each user input below, identify any cognitive distortions present "
        "from the following list: all-or-nothing thinking, overgeneralization, mental filter, "
        "disqualifying the positive, jumping to conclusions, magnification, emotional reasoning, "
        "should statements, labeling, personalization, blaming others.\n\n"
        "Return a JSON-style Python list of strings, where each item is the distortion(s) "
        "for the corresponding input. If none are found, return \"none\". Example:\n"
        "[\"should statements\", \"none\", \"labeling, personalization\"]\n\n"
        "Inputs:\n" +
        "\n".join([f"{i+1}. {text}" for i, text in enumerate(batch_inputs)])
    )

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=1024
        )
        # extract inside code block if wrapped in ```python ... ```
        output_text = response['choices'][0]['message']['content'].strip()
        match = re.search(r"```(?:python)?\s*(\[[\s\S]*?\])\s*```", output_text)
        if match:
            output_text = match.group(1)
        return ast.literal_eval(output_text)
    except Exception as e:
        print("⚠️ Error parsing output:", e)
        return ["error"] * len(batch_inputs)

# STEP 6: Run in batches
batch_size = 20
results = []

for i in tqdm(range(0, len(df), batch_size)):
    batch = df['input'].iloc[i:i+batch_size].tolist()
    distortions = get_distortions_batch(batch)
    results.extend(distortions)
    time.sleep(3)  # <-- Added 15-second delay

# STEP 7: Add results to DataFrame
df['cognitive_distortions'] = results

# STEP 8: Save to new CSV
df.to_csv("Psych8k_with_distortions_sample.csv", index=False)

# STEP 9: Preview
df.head()

# Assume `results` is your list of distortion predictions
# Pad it if it's shorter than the DataFrame
if len(results) < len(df):
    print(f"Patching: {len(df) - len(results)} missing values detected.")
    results += ["not_processed"] * (len(df) - len(results))
elif len(results) > len(df):
    print(f"Warning: {len(results) - len(df)} extra values. Trimming.")
    results = results[:len(df)]

# Now assign safely
df["cognitive_distortions"] = results

df.to_csv("Psych8k_with_distortions_final.csv", index=False)

df.head(50)

import pandas as pd
import json

# Step 1: Load the Psych8k dataset with cognitive distortions
psych8k_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/Psych8k/Psych8k_with_distortions.csv"  # Path to the CSV file
psych8k_df = pd.read_csv(psych8k_path)

# Step 4: Reformat the dataset into a standardized conversation structure
conversations = []  # To store the JSON structure for all conversations
for idx, row in psych8k_df.iterrows():
    user_content = row["input"]
    therapist_content = row["output"]
    distortions = row["cognitive_distortions"]

    # Build the conversation object
    conversation = {
        "conversation_id": f"psych8k_{idx}",  # Unique conversation ID
        "turns": [
            {"role": "user", "content": user_content},
            {"role": "therapist", "content": therapist_content}
        ],
        "metadata": {
            "source": "psych8k",
            "type": "single-turn",  # Psych8k is single-turn conversations
            "turn_count": 2,  # Each conversation has 2 turns (user + therapist)
            "cognitive_distortions": distortions.split(", ") if isinstance(distortions, str) else []  # Split distortions into a list
        }
    }
    conversations.append(conversation)

# Step 5: Save the preprocessed dataset as a JSON file
output_path = "Preprocessed_Psych8k_with_Cognitive_Distortions.json"  # Output filename
with open(output_path, "w") as json_file:
    json.dump(conversations, json_file, indent=4)

print(f"Preprocessed dataset saved to: {output_path}")

# Step 3: Reformat dataset into standardized conversation structure

def create_conversation(row, conversation_id):
    # Create a conversation object
    return {
        "conversation_id": f"psych_{conversation_id}",
        "turns": [
            {"role": "user", "content": row['input']},
            {"role": "therapist", "content": row['output']}
        ],
        "metadata": {
            "source": "psych8k",
            "type": "single-turn",
            "turn_count": 2,
            "cognitive_distortions": []  # Initialize with an empty list
        }
    }

# Apply the transformation
conversations = [create_conversation(row, idx) for idx, row in psych8k_df.iterrows()]

# Check an example conversation
print("Example of a reformatted conversation:")
print(conversations[0])

psych8k_df.info()

# Step 5: Save the preprocessed dataset as a JSON file
import json

# Define output path
output_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/Psych8k/Preprocessed_Psych8k.json"

# Save conversations to JSON
with open(output_path, 'w') as json_file:
    json.dump(conversations, json_file, indent=4)

print(f"Preprocessed dataset saved to: {output_path}")

"""# Data Preprocessing for CBT Conversations Dataset"""

# Step 1: Load the dataset and inspect
import json

# Define the dataset path
cbt_dataset_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/SynthDataset for FTing/cleaned_cbt_therapy_dataset.json"

# Load the dataset
with open(cbt_dataset_path, 'r') as f:
    cbt_data = json.load(f)

# Check the number of conversations and display an example
print(f"Total conversations in the dataset: {len(cbt_data)}")
print("Example of a conversation:")
print(json.dumps(cbt_data[0], indent=4))

# Step 2: Validate and clean conversations

# Remove incomplete conversations (less than 2 turns)
cbt_data = [conv for conv in cbt_data if len(conv.get('conversations', [])) >= 2]

# Standardize role labels
def standardize_roles(conversation):
    for turn in conversation['conversations']:
        if turn['role'].lower() in ['user', 'client', 'patient']:
            turn['role'] = 'user'
        elif turn['role'].lower() in ['therapist', 'counselor']:
            turn['role'] = 'therapist'
    return conversation

cbt_data = [standardize_roles(conv) for conv in cbt_data]

# Verify cleaned data
print(f"Conversations after cleaning: {len(cbt_data)}")
print("Example of a cleaned conversation:")
print(json.dumps(cbt_data[0], indent=4))

# Step 3: Structure conversations into a consistent format

def structure_conversation(conversation, conversation_id):
    return {
        "conversation_id": f"cbt_{conversation_id}",
        "turns": [
            {"role": turn['role'], "content": turn['content']}
            for turn in conversation['conversations']
        ],
        "metadata": {
            "source": "cbt",
            "type": "multi-turn",
            "turn_count": len(conversation['conversations'])
        }
    }

# Apply the transformation
structured_conversations = [structure_conversation(conv, idx) for idx, conv in enumerate(cbt_data)]

# Check an example structured conversation
print("Example of a structured conversation:")
print(json.dumps(structured_conversations[0], indent=4))

# Step 4: Detect cognitive distortions and add tags
import re

# Cognitive Distortions Dictionary
cognitive_distortions = {
    "all_or_nothing_thinking": [
        "always", "never", "completely", "totally", "entirely", "perfect", "failure", "ruined", "no middle ground", "black and white"
    ],
    "overgeneralization": [
        "everyone", "nobody", "all the time", "never works out", "everywhere", "forever", "it’s bound to happen", "this always happens"
    ],
    "mental_filter": [
        "only see the negative", "focus on flaws", "ignore the positive", "nothing good", "all I notice is the bad", "all problems, no solutions"
    ],
    "disqualifying_the_positive": [
        "that doesn’t count", "it was just luck", "anyone could have done it", "I don’t deserve credit", "it’s not a big deal", "they’re just being nice"
    ],
    "jumping_to_conclusions_mind_reading": [
        "I know they think I’m a failure", "they must hate me", "they’re judging me", "everyone sees I’m worthless", "I can tell they don’t like me"
    ],
    "jumping_to_conclusions_fortune_telling": [
        "it will never work", "I’m doomed", "this is going to fail", "I just know it’s going to end badly", "I can’t see any good outcome"
    ],
    "magnification_or_minimization": [
        "it’s the end of the world", "this is a total disaster", "it’s nothing special", "downplay my success", "make a mountain out of a molehill", "it’s not that important"
    ],
    "emotional_reasoning": [
        "I feel it, so it must be true", "if I’m scared, there’s real danger", "I feel worthless, so I am worthless", "my emotions are facts", "I feel guilty, so I must have done something wrong"
    ],
    "should_statements": [
        "I should be able to handle this", "I must do better", "I have to be perfect", "I shouldn’t feel this way", "I ought to be stronger", "I shouldn’t make mistakes"
    ],
    "labeling": [
        "I’m a loser", "I’m worthless", "I’m a failure", "I’m useless", "I’m stupid", "I’m incompetent"
    ],
    "personalization": [
        "it’s all my fault", "I caused this", "I messed up everyone’s day", "I’m responsible for everything", "if only I had done better"
    ],
    "blaming_others": [
        "it’s all their fault", "they made this happen", "they ruined everything", "they are the reason I’m like this", "I wouldn’t feel this way if not for them"
    ]
}

# Function to detect cognitive distortions
def detect_cognitive_distortions(conversation):
    detected_distortions = set()
    for turn in conversation['turns']:
        if turn['role'] == 'user':  # Only analyze the user's turns
            for distortion, phrases in cognitive_distortions.items():
                if any(re.search(rf'\b{phrase}\b', turn['content'], re.IGNORECASE) for phrase in phrases):
                    detected_distortions.add(distortion)
    return list(detected_distortions)

# Add detected cognitive distortions to metadata
for conversation in structured_conversations:
    distortions = detect_cognitive_distortions(conversation)
    conversation['metadata']['cognitive_distortions'] = distortions

# Check an example with added cognitive distortion metadata
print("Example conversation with cognitive distortion tags:")
print(json.dumps(structured_conversations[0], indent=4))

print(json.dumps(structured_conversations[974], indent=4))

# Step 5: Save the preprocessed dataset as a JSON file

# Define output path
output_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/SynthDataset for FTing/Preprocessed_CBT_Dataset_Enriched.json"

# Save structured conversations to JSON
with open(output_path, 'w') as json_file:
    json.dump(structured_conversations, json_file, indent=4)

print(f"Preprocessed dataset saved to: {output_path}")

"""# Dataset Merging"""

/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/Psych8k/Psych8k_with_distortions.csv
/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/SynthDataset for FTing/Preprocessed_CBT_Dataset_Enriched.json

import pandas as pd
import json

# Step 1: Load the Psych8k dataset
psych8k_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/Psych8k/Psych8k_with_distortions.csv"
psych8k_df = pd.read_csv(psych8k_path)

# Step 2: Convert Psych8k dataset into JSON format of conversations
psych8k_conversations = []
for idx, row in psych8k_df.iterrows():
    conversation = {
        "conversation_id": f"psych8k_{idx}",
        "turns": [
            {"role": "user", "content": row["input"]},
            {"role": "therapist", "content": row["output"]}
        ],
        "metadata": {
            "source": "psych8k",
            "type": "single-turn",
            "turn_count": 2,
            "cognitive_distortions": row["cognitive_distortions"].split(", ") if isinstance(row["cognitive_distortions"], str) else ["none"]
        }
    }
    psych8k_conversations.append(conversation)

# Step 3: Load the CBT dataset
cbt_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/SynthDataset for FTing/Preprocessed_CBT_Dataset_Enriched.json"
with open(cbt_path, "r") as cbt_file:
    cbt_conversations = json.load(cbt_file)

# Step 4: Validate and standardize CBT dataset
for conversation in cbt_conversations:
    # Add "none" if cognitive distortions are empty
    if not conversation["metadata"].get("cognitive_distortions"):
        conversation["metadata"]["cognitive_distortions"] = ["none"]
    # Ensure unique conversation IDs
    conversation["conversation_id"] = f"cbt_{conversation['conversation_id']}"

# Step 5: Merge the datasets
merged_conversations = psych8k_conversations + cbt_conversations

# Step 6: Save the merged dataset as a JSON file
output_path = "/content/drive/MyDrive/ACADEMICS/FYP/4. Implementation/Datasets/Psych8k_and_CBT_Conversation_Dataset.json"
with open(output_path, "w") as output_file:
    json.dump(merged_conversations, output_file, indent=4)

print(f"Merged dataset saved to: {output_path}")

print(merged_conversations[500])